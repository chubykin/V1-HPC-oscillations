{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import scipy.stats as sstat\n",
    "import scipy.signal as ssig\n",
    "import h5py\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import re\n",
    "\n",
    "# import ephys_unit_analysis as ena\n",
    "import mz_ephys_unit_analysis as mz_ena\n",
    "\n",
    "#import resampy\n",
    "import fnmatch\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP probe inserted through V1 and hippo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = 'Neuropixels'\n",
    "channel_groups = mz_ena.get_channel_depth(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files = []\n",
    "matches = [] # list of experiment folders\n",
    "source_folder = r\"G:\\Neuropixels\\NMDA_V1HPC\\SORTED\"\n",
    "\n",
    "for root, dirnames, filenames in os.walk(source_folder):\n",
    "    for filename in fnmatch.filter(filenames, '*rez.mat'):\n",
    "        for filename in fnmatch.filter(filenames, '*cluster_group.tsv'):#For newer phy2 GUI, .tsv instead of .csv files\n",
    "            \n",
    "            # change this before running otherwise there will be none\n",
    "            if str('et') in root: \n",
    "                if (str('noisy') not in root):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "                    root_files.append(root)\n",
    "                    print (root)\n",
    "\n",
    "print('\\nIMPORTANT: This has \"cluster_group.tsv\" already appended to the matches list')\n",
    "print('How many files?', len(matches))\n",
    "print('How many root files?', len(root_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_units_or_good = 1   # if 0--manually sorted good units, if 1--all units from KS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to check and make sure the splitting in the function below is correct!\n",
    "f = root_files[0]\n",
    "\n",
    "f.split('\\\\')[-1].split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = []\n",
    "df_rez = []\n",
    "\n",
    "for path in root_files:\n",
    "    cluster_path = os.path.join(path, 'cluster_KSLabel.tsv')    \n",
    "    #------------------------------------------------------------------------------------\n",
    "    # These probably change depending on the file naming I used during the recording\n",
    "    situation = path.split('\\\\')[-1].split('_')[0]\n",
    "    group = path.split('\\\\')[-1].split('_')[1]\n",
    "    et_num = path.split('\\\\')[-1].split('_')[2]   # what et\n",
    "    cc_num = path.split('\\\\')[-1].split('_')[3]   # what cc\n",
    "    #------------------------------------------------------------------------------------\n",
    "    cluster_groups = pd.read_csv(cluster_path, sep = '\\t')\n",
    "    #------------------------------------------------------------------------------------\n",
    "    if all_units_or_good == 0:\n",
    "        good = cluster_groups[cluster_groups['group'] == 'good'].cluster_id.values\n",
    "    elif all_units_or_good == 1:\n",
    "        good = cluster_groups[cluster_groups['KSLabel'] == 'good'].cluster_id.values\n",
    "    #------------------------------------------------------------------------------------\n",
    "    spike_clusters = np.load(os.path.join(path, 'spike_clusters.npy'))\n",
    "    spike_times = np.load(os.path.join(path, 'spike_times.npy'))\n",
    "    templates = np.load(os.path.join(path, 'templates.npy'))\n",
    "    spike_templates = np.load(os.path.join(path, 'spike_templates.npy'))\n",
    "    #------------------------------------------------------------------------------------\n",
    "    foo = pd.DataFrame({'situ':situation,\n",
    "                        'group': group,\n",
    "                        'et': et_num,\n",
    "                        'cc':cc_num,\n",
    "                        'cluster_id':spike_clusters.flatten(),\n",
    "                        'times':spike_times.flatten()/30000.0, \n",
    "                        'templates':spike_templates.flatten(),\n",
    "                        'path':f})        \n",
    "    data_df.append(foo)\n",
    "    #------------------------------------------------------------------------------------\n",
    "    foo_1 = foo[foo.cluster_id.isin(good)]\n",
    "    df_rez.append(foo_1)\n",
    "    #------------------------------------------------------------------------------------\n",
    "data_df = pd.concat(data_df, axis=0, ignore_index=True)\n",
    "df_rez = pd.concat(df_rez, axis=0, ignore_index=True)\n",
    "\n",
    "print('total units df shape:', data_df.shape)\n",
    "print('\"good\" units df shape:', df_rez.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['cuid'] =  data_df.et.astype(str) + str('_') + data_df.cluster_id.astype(str)\n",
    "df_rez['cuid'] =  df_rez.et.astype(str) + str('_') + df_rez.cluster_id.astype(str)\n",
    "\n",
    "print(\"Total units:\", data_df['cuid'].nunique())\n",
    "print(\"Good units:\", df_rez['cuid'].nunique())\n",
    "\n",
    "df_rez.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep going from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_trials = 30\n",
    "\n",
    "trials_number = tot_trials # ~~~~~~~~~~~~~~~~~~~~~~ IMPORTANT THIS IS CORRECT ~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "trial_length = 3.0 #this is the length of the recording in OpenEphys (the yellow highlight)\n",
    "th_bin = 0.01\n",
    "\n",
    "ls_rawcount = []\n",
    "ls_lowspikecount = []\n",
    "ls_refract_violators = []\n",
    "ls_lowamp_waveforms = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run these to add a stim column (operant, sf-tuning, ori-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df['trial']=(data_df.times//trial_length).astype(int)\n",
    "# data_df['stim']=data_df.trial.map(dict(zip(np.arange(trials_number),([int(i) for i in overall_order]))))\n",
    "\n",
    "# df_rez['trial']=(df_rez.times//trial_length).astype(int)\n",
    "# df_rez['stim']=df_rez.trial.map(dict(zip(np.arange(trials_number),([int(i) for i in overall_order]))))\n",
    "# df_rez.head()\n",
    "\n",
    "# For recordings where only 1 stimulus is shown (aka novel recordings, or pre, or post training)\n",
    "# adding the \"stim\" column keeps the rest of the code able to run\n",
    "data_df['trial']=(data_df.times//trial_length).astype(int)\n",
    "data_df['stim']=0\n",
    "df_rez['trial']=(df_rez.times//trial_length).astype(int)\n",
    "df_rez['stim']=0\n",
    "df_rez.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the spikes and tmt dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_spikes = []\n",
    "ls_tmt = []\n",
    "\n",
    "i=0\n",
    "\n",
    "num_units = df_rez['cuid'].nunique()\n",
    "\n",
    "for iii, unit in enumerate(df_rez['cuid'].unique()): ##### I changed this from df_rez to data_df to check the units\n",
    "    cuid = str(unit)\n",
    "    tmp2 = df_rez[(df_rez.cuid == unit)]             ##### I changed this from df_rez to data_df to check the units\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    cluster_id = tmp2.cluster_id.values[0]\n",
    "    situ = tmp2.situ.values[0]\n",
    "    group = tmp2.group.values[0]\n",
    "    et = tmp2.et.values[0]\n",
    "    cc = tmp2.cc.values[0]\n",
    "    path = tmp2.path.values[0]\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    try:\n",
    "        for stim_id in tmp2.stim.unique():\n",
    "            tmp3=tmp2[tmp2.stim==stim_id]\n",
    "            tmt, depth, ch_idx = mz_ena.ksort_get_tmt(tmp3, cluster_id, templates, channel_groups)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            df = mz_ena.getRaster_kilosort(tmp3, unit, trial_length) \n",
    "            trials_number_not_empty = len(df.trial.unique())    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            df_spikes_tmp = pd.DataFrame({'cluster_id': cluster_id, \n",
    "                                          'spikes': tmp3.times.values,\n",
    "                                          'trial':df.trial,\n",
    "                                          'stim_id':stim_id,\n",
    "                                          'trial_spikes':df.times,\n",
    "                                          'depth':depth,\n",
    "                                          'situ':situ,\n",
    "                                          'group':group,\n",
    "                                          'et':et,\n",
    "                                          'cc': cc,\n",
    "                                          'cuid': cuid,\n",
    "                                          'path':path})\n",
    "            ls_spikes.append(df_spikes_tmp)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            df_tmt_tmp = pd.DataFrame({'tmt': tmt,\n",
    "                                       'stim_id':stim_id,\n",
    "                                       'depth':depth,\n",
    "                                       'situ':situ,\n",
    "                                       'group':group,\n",
    "                                       'et':et,\n",
    "                                       'cc': cc,\n",
    "                                       'cuid': cuid,\n",
    "                                       'path':path})\n",
    "            ls_tmt.append(df_tmt_tmp)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    except:\n",
    "        i+=1\n",
    "        continue\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    if iii%200 == 0:\n",
    "        print('done with {0} out of {1}'.format(iii, num_units))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df_spikes = pd.concat(ls_spikes)\n",
    "df_tmt = pd.concat(ls_tmt)\n",
    "print(\"Total errors: {0} out of {1} units\".format(i,num_units))\n",
    "print(\"ALL DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spikes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_group(row):\n",
    "#     nmda_ls = ['et1710', 'et1700', 'et1570', 'et1520', 'et171', 'et170', 'et157', 'et152']\n",
    "#     if row['et'] in nmda_ls:\n",
    "#         return \"nmda\"\n",
    "#     elif \n",
    "#         return \"sham\"\n",
    "\n",
    "def label_region(row):\n",
    "    if row['depth'] <= 1100:\n",
    "        return 'v1'\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_spikes['group'] = df_spikes.apply(lambda row: label_group(row), axis=1)\n",
    "df_spikes['region'] = df_spikes.apply(lambda row: label_region(row), axis=1)\n",
    "\n",
    "df_spikes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmt['group'] = df_tmt.apply(lambda row: label_group(row), axis=1)\n",
    "df_tmt['region'] = df_tmt.apply(lambda row: label_region(row), axis=1)\n",
    "\n",
    "df_tmt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding to the waveforms (tmt) dataFrame\n",
    "Based on Yu's code and outputs the spike width, peak-to-trough amplitude, and the overall rs/fs classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resampy\n",
    "import scipy\n",
    "import scipy.signal as ssig\n",
    "\n",
    "def gaus(x,a,x0,sigma):\n",
    "    return a*np.exp(-(x-x0)**2/(2*sigma**2))\n",
    "\n",
    "result = df_tmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_width = {}\n",
    "tr2peak = {}\n",
    "neuron_type = {}\n",
    "tp_dic = {}\n",
    "w_dic = {}\n",
    "ls = []\n",
    "ls2 = []\n",
    "ls3 = []\n",
    "\n",
    "num_units = result['cuid'].nunique()\n",
    "\n",
    "for ii,cuid in enumerate(result.cuid.unique()[:]):    \n",
    "    if ii%200 == 0:\n",
    "        print('done with {0} out of {1}'.format(ii, num_units))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    tmt_data = np.array(result[(result['cuid'] == cuid)].tmt)\n",
    "\n",
    "    y = resampy.resample( tmt_data[::-1] , 1 ,10,  filter='sinc_window',\n",
    "                                    num_zeros=10, precision=5, window=ssig.hann)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #trough-to-peak\n",
    "    trough_idx = y.argmin()\n",
    "    peak_idx = y[:y.argmin()].argmax()\n",
    "    tp = abs((trough_idx - peak_idx)/300.0)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    x = np.arange(y.size)\n",
    "    y_gaus = y*(-1)\n",
    "    popt,pcov = scipy.optimize.curve_fit(gaus,x,y_gaus,p0=[0.2, y.argmin(), 10])\n",
    "    fwhm = popt[-1]/300*2.355\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    #width of spike    \n",
    "    f,pxx = ssig.welch(tmt_data, fs=30000,  nfft=5096,  nperseg=48,\n",
    "                          return_onesided=True, scaling='spectrum')\n",
    "\n",
    "    df = np.vstack((f, pxx))\n",
    "    df = pd.DataFrame(df)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    idx = df.T[1].idxmax()\n",
    "    if not np.isnan(idx):\n",
    "        w = df.T[0][idx]\n",
    "        w = 1/w*1000.0\n",
    "        ls.append(tp)\n",
    "        ls2.append(w)\n",
    "        spk_width[cuid] = w\n",
    "        tr2peak[cuid] = tp\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        if tp<=0.45: #can add something for width here as well, but then you'll have 'fs', 'rs', and 'un' units\n",
    "            neuron_type[cuid] = 'fs'\n",
    "        elif tp>0.45:\n",
    "            neuron_type[cuid] = 'rs'\n",
    "        else:\n",
    "            neuron_type[cuid] = 'un'\n",
    "        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # p/t ratio\n",
    "        ptr = result[(result['cuid'] == cuid)]['tmt'].max()/result[(result['cuid'] == cuid)]['tmt'].min()\n",
    "        ls3.append(abs(ptr))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~        \n",
    "df_tmt['n_type'] = df_tmt.cuid.map(neuron_type)\n",
    "df_tmt['trough2peak'] = df_tmt.cuid.map(tr2peak)\n",
    "df_tmt['spk_width'] = df_tmt.cuid.map(spk_width)\n",
    "\n",
    "df_tmt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rs units: {0}'.format(df_tmt[df_tmt['n_type'] == 'rs'].cuid.nunique()))\n",
    "print('Number of fs units: {0}'.format(df_tmt[df_tmt['n_type'] == 'fs'].cuid.nunique()))\n",
    "print('Number of un units: {0}'.format(df_tmt[df_tmt['n_type'] == 'un'].cuid.nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorder the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a last reordering of the columns for easy viewing -- spikes df\n",
    "cols = ['cluster_id', 'spikes', 'trial', 'stim_id', 'trial_spikes', \n",
    "        'depth', 'cuid', 'situ', 'group', 'region', 'et', 'cc','path']\n",
    "\n",
    "df_spikes = df_spikes[cols]\n",
    "\n",
    "df_spikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a last reordering of the columns for easy viewing -- waveforms df\n",
    "cols = ['tmt', 'n_type', 'trough2peak', 'spk_width', 'stim_id', 'cuid', \n",
    "        'depth', 'region', 'situ', 'group', 'cc', 'et', 'path']\n",
    "\n",
    "df_tmt = df_tmt[cols]\n",
    "\n",
    "df_tmt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Spikes and Waveform dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spikes.to_pickle(r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\spikes_df.pkl\")\n",
    "\n",
    "df_tmt.to_pickle(r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\waveforms_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the PSTH dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rez.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_psth = []\n",
    "i=0\n",
    "\n",
    "num_units = df_rez['cuid'].nunique()\n",
    "\n",
    "for iii, unit in enumerate(df_rez['cuid'].unique()): ##### I changed this from df_rez to data_df to check the units\n",
    "    cuid = str(unit)\n",
    "    tmp2 = df_rez[(df_rez.cuid == unit)]             ##### I changed this from df_rez to data_df to check the units\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    cluster_id = tmp2.cluster_id.values[0]\n",
    "    situ = tmp2.situ.values[0]\n",
    "    group = tmp2.group.values[0]\n",
    "    et = tmp2.et.values[0]\n",
    "    cc = tmp2.cc.values[0]\n",
    "    path = tmp2.path.values[0]\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    try:\n",
    "        tmt, depth, ch_idx = mz_ena.ksort_get_tmt(tmp2, cluster_id, templates, channel_groups)\n",
    "    except:\n",
    "        i = i+1        \n",
    "        continue    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    if iii%200 == 0: #think of this as a loading bar to know for far it has run\n",
    "        print('done with {0} out of {1}'.format(iii, num_units))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    for stim_id in tmp2.stim.unique():\n",
    "        tmp3=tmp2[tmp2.stim==stim_id]\n",
    "        df = mz_ena.getRaster_kilosort(tmp3, unit, trial_length) \n",
    "        trials_number_not_empty = len(df.trial.unique())    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        h, ttr = mz_ena.PSTH(df.times, th_bin, trial_length, trials_number_not_empty)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        zscore = sstat.mstats.zscore(h)\n",
    "        mean = np.mean(h[0:50])#The Baseline period. Be sure it matches time course of experiments##\n",
    "        if mean<=0:\n",
    "            std=1\n",
    "        else:\n",
    "            std = np.std(h[0:50])#The Baseline period. Be sure it matches time course of experiments##\n",
    "        ztc = (h - mean)/std\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        df_psth_tmp = pd.DataFrame({'times':ttr,\n",
    "                                    'stim_id': stim_id,\n",
    "                                    'Hz':h,\n",
    "                                    'cluster_id': cluster_id,\n",
    "                                    'depth': depth,\n",
    "                                    'zscore':zscore, \n",
    "                                    'ztc':ztc,\n",
    "                                    'situ':situ,\n",
    "                                    'group':group,\n",
    "                                    'et':et,\n",
    "                                    'cc':cc,\n",
    "                                    'cuid':cuid,\n",
    "                                    'path':path})\n",
    "        ls_psth.append(df_psth_tmp)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df_psth = pd.concat(ls_psth)\n",
    "print(\"Total errors: {0} out of {1} units\".format(i,num_units))\n",
    "print(\"ALL DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min unit depth on probe:', df_psth['depth'].min())\n",
    "print('Max unit depth on probe:',df_psth['depth'].max())\n",
    "\n",
    "# print('\\n',np.sort(df_psth['depth'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = df_psth[df_psth['zscore'] < 20]\n",
    "\n",
    "print('# good units: %d' % df_psth['cuid'].nunique())\n",
    "print('# good units w/ z-score < 20: %d' % null_df['cuid'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the PSTH dataFrame with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# null_df['group'] = null_df.apply(lambda row: label_group(row), axis=1)\n",
    "null_df['region'] = null_df.apply(lambda row: label_region(row), axis=1)\n",
    "\n",
    "null_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reordering the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a last reordering of the columns for easy viewing -- Reward trials\n",
    "cols = ['stim_id', 'times', 'cuid', 'depth', 'Hz', 'zscore', 'ztc', \n",
    "        'region', 'situ', 'group', 'cc', 'et', 'cluster_id', 'path']\n",
    "null_df = null_df[cols]\n",
    "null_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataframe and plot elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "null_df.to_pickle(r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\psth_df.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
