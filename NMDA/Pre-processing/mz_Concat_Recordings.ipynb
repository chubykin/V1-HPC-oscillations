{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import glob\n",
    "\n",
    "from glob import glob\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import sys\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "# import phy\n",
    "# import Python3_OpenEphys_con2dat_align_v2 as c2d\n",
    "# import Python3_new_OpenEphys_orig as orig\n",
    "import Binary as bn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyCAR(data, pr=0): #this input data should be in the form (channels,samples) ie (384,7400)\n",
    "    \n",
    "    if pr==1:\n",
    "        print('Shape before: %s' %str(data.shape))\n",
    "\n",
    "    ch_median = np.median(data,axis=1) #subtract median across each channel\n",
    "    data = data-ch_median[:,None]\n",
    "    time_median = np.median(data, axis=0) #subtract median across each time sample\n",
    "    data = (data.T-time_median[:,None]).T\n",
    "    \n",
    "    if pr==1:\n",
    "        print('Shape after: %s' %str(data.shape))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the folder location first\n",
    "\n",
    "Change the following before running:\n",
    "1. folder_name\n",
    "\n",
    "Double check that these are correct:\n",
    "1. in_path\n",
    "2. out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy the folder name here: pre_et306_cc033657_2022-03-15_11-40-00\n",
      "\n",
      "File name: pre_et306_cc033657_2022-03-15_11-40-00\n",
      "\n",
      "# of experiments: 1\n"
     ]
    }
   ],
   "source": [
    "in_path = r\"D:\\mz_Data\\RECORDING_DATA\\HDAC_data\"\n",
    "out_path = r\"D:\\mz_Data\\RECORDING_DATA\\HDAC_data\\SORTED_DATA\"\n",
    "\n",
    "folder_name = input(\"Copy the folder name here: \")    #make sure to change this before running!!\n",
    "\n",
    "working_path = in_path + \"\\\\\" + folder_name\n",
    "print('\\nFile name: ' + folder_name)\n",
    "\n",
    "\n",
    "folders = [os.path.join(working_path,fld) for fld in os.listdir(working_path) if 'experiment' in fld]\n",
    "\n",
    "print('\\n# of experiments: %d' %len(folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate all of the recordings to a single .dat file\n",
    "\n",
    "### What happens?\n",
    "Essentially in the cell above it created a new dictionary -- \"Data\" -- that is structured as follows: {Process: {Experiment: {recording} } } where the recording is split into each trial and the data associated to it\n",
    "\n",
    "ie. {'100': {'0': {9: memmap(the_data)} } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "Unit='int16'\n",
    "ChannelMap=[]\n",
    "\n",
    "for Folder in folders:\n",
    "\n",
    "    #File locations for use on windows\n",
    "    Files = sorted(glob(Folder + r'\\**\\Neuropix-PXI-100.0\\continuous.dat', recursive=True))\n",
    "    InfoFiles = sorted(glob(Folder + r'\\*\\structure.oebin'))\n",
    "    ttlFiles=sorted(glob(Folder + r'\\**\\Neuropix-PXI-100.0\\TTL_1\\timestamps.npy', recursive=True))\n",
    "    tssFiles=sorted(glob(Folder + r'\\**\\Neuropix-PXI-100.0\\timestamps.npy', recursive=True))\n",
    "    \n",
    "    sorted_index=[f.split('\\\\')[-5][9:] for f in sorted(ttlFiles)]\n",
    "    ttl=[np.load(ttl)[0] for ttl in sorted(ttlFiles)]\n",
    "    tss=[np.load(tss) for tss in sorted(tssFiles)]\n",
    "    tmp=np.array(list(itertools.zip_longest(*tss, fillvalue=np.nan))).T\n",
    "    sliceat=np.where((tmp-np.array(ttl)[:,None])==0)[1]\n",
    "    \n",
    "    Data, Rate = {}, {}\n",
    "    for F,File in enumerate(Files):\n",
    "        Exp, Rec, _, Proc = File.split('\\\\')[-5:-1]\n",
    "        Exp = str(int(Exp[10:])-1)\n",
    "        Rec = str(int(Rec[9:])-1)\n",
    "        Proc = Proc.split('.')[0].split('-')[-1]\n",
    "        if Proc not in Data.keys(): Data[Proc], Rate[Proc] = {}, {}\n",
    "        if '_' in Proc: Proc = Proc.split('_')[0]\n",
    "\n",
    "#         print('Loading recording', int(Rec)+1, '...')\n",
    "\n",
    "        if Exp not in Data[Proc]: Data[Proc][Exp] = {}\n",
    "        Data[Proc][Exp][Rec] = np.memmap(File, dtype='int16', mode='c')\n",
    "        \n",
    "        Info = bn.literal_eval(open(InfoFiles[F]).read())\n",
    "        ProcIndex = [Info['continuous'].index(_) for _ in Info['continuous']\n",
    "                     if str(_['recorded_processor_id']) == Proc][0]\n",
    "\n",
    "        ChNo = Info['continuous'][ProcIndex]['num_channels']\n",
    "        if Data[Proc][Exp][Rec].shape[0]%ChNo:\n",
    "            print('Rec', Rec, 'is broken')\n",
    "            del(Data[Proc][Exp][Rec])\n",
    "            continue\n",
    "\n",
    "        SamplesPerCh = Data[Proc][Exp][Rec].shape[0]//ChNo\n",
    "        Data[Proc][Exp][Rec] = Data[Proc][Exp][Rec].reshape((SamplesPerCh, ChNo))\n",
    "        Rate[Proc][Exp] = Info['continuous'][ProcIndex]['sample_rate']\n",
    "\n",
    "    for Proc in Data.keys():\n",
    "        for Exp in Data[Proc].keys():\n",
    "            if Unit.lower() in ['uv', 'mv']:\n",
    "                ChInfo = Info['continuous'][ProcIndex]['channels']\n",
    "                Data[Proc][Exp] = bn.BitsToVolts(Data[Proc][Exp], ChInfo, Unit)\n",
    "\n",
    "            if ChannelMap: Data[Proc][Exp] = bn.ApplyChannelMap(Data[Proc][Exp], ChannelMap)\n",
    "\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is to check the length of each recording in seconds\n",
    "you'll find that the first 5 (from 0-4) are really short, which is due to how the TTL signal starts each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.05243333333333333\n",
      "1 0.06826666666666667\n",
      "2 0.034133333333333335\n",
      "3 0.06826666666666667\n",
      "4 0.0896\n",
      "5 3.0108\n",
      "6 3.0104\n",
      "7 2.99\n",
      "8 3.0104\n",
      "9 3.0004\n",
      "10 3.0208\n",
      "11 2.9904\n",
      "12 2.98\n",
      "13 3.0104\n",
      "14 3.0104\n",
      "15 3.0104\n",
      "16 3.0004\n",
      "17 2.98\n",
      "18 3.0004\n",
      "19 3.0004\n",
      "20 3.0103666666666666\n",
      "21 3.0208\n",
      "22 2.9994\n",
      "23 2.99\n",
      "24 3.0104\n",
      "25 2.99\n",
      "26 3.0104\n",
      "27 2.98\n",
      "28 3.0108\n",
      "29 3.0084\n",
      "30 3.0004\n",
      "31 3.010466666666667\n",
      "32 3.0108\n",
      "33 3.0104\n"
     ]
    }
   ],
   "source": [
    "Data['100']['0']={int(k):v for k,v in Data['100']['0'].items()}\n",
    "for k,v in sorted(Data['100']['0'].items()):\n",
    "    print(k,v.shape[0]/30000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is to check shape of each recording data frame\n",
    "It creates a new dictionary -- \"Data_align\" -- and shows that each trial has 384 channels recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (1414, 384)\n",
      "2 (1588, 384)\n",
      "3 (910, 384)\n",
      "4 (1256, 384)\n",
      "5 (2101, 384)\n",
      "6 (89990, 384)\n",
      "7 (89920, 384)\n",
      "8 (89608, 384)\n",
      "9 (89927, 384)\n",
      "10 (89420, 384)\n",
      "11 (90029, 384)\n",
      "12 (89531, 384)\n",
      "13 (89336, 384)\n",
      "14 (90048, 384)\n",
      "15 (89742, 384)\n",
      "16 (90028, 384)\n",
      "17 (89737, 384)\n",
      "18 (89247, 384)\n",
      "19 (89652, 384)\n",
      "20 (89946, 384)\n",
      "21 (90048, 384)\n",
      "22 (90046, 384)\n",
      "23 (89833, 384)\n",
      "24 (89549, 384)\n",
      "25 (89847, 384)\n",
      "26 (89551, 384)\n",
      "27 (89846, 384)\n",
      "28 (89342, 384)\n",
      "29 (89959, 384)\n",
      "30 (90000, 384)\n",
      "31 (89562, 384)\n",
      "32 (89857, 384)\n",
      "33 (89918, 384)\n",
      "34 (90028, 384)\n"
     ]
    }
   ],
   "source": [
    "Data_align=dict()\n",
    "for i,rec in enumerate(sorted_index):\n",
    "    Data_align[rec]=Data['100']['0'][int(rec)-1][sliceat[i]:]\n",
    "\n",
    "\n",
    "Data_align={int(k):v for k,v in Data_align.items()}\n",
    "for k,v in sorted(Data_align.items()):\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctly pad each recording (NOT the first 5 short ones, tho)\n",
    "\n",
    "First it applies the ___Common Average Referencing___, then does the rest\n",
    "\n",
    "It's correctly aligning them by having each trial be an identical length\n",
    "\n",
    "This is essentially doing two things\n",
    "1. If any trial is longer than the 3 sec length of recording, only the first 3*30000 samples will be used\n",
    "2. If any trial is shorter than the 3 sec length of recording, it will be 0 padded to the 3*30000 samples\n",
    "\n",
    "\n",
    "**-----Important-----**\n",
    "\n",
    "If you get an error similar to \"MemoryError: Unable to allocate 38.6 GiB for an array with shape (13500000, 384) and data type float64\", this means the jupyter notebook has reached it's maximum memory working with the data.\n",
    "\n",
    "You need to Restart the Kernel and clear all output to fix this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of each recording: 3 seconds\n",
      "target samples per recording: 90000\n",
      "Final concat shape: (2610000, 384) -- should be (samp,ch)\n"
     ]
    }
   ],
   "source": [
    "length_recording = 3\n",
    "sample_rate = 30000\n",
    "\n",
    "trial_samples = length_recording*sample_rate\n",
    "\n",
    "print('length of each recording: %d seconds' %length_recording)\n",
    "print('target samples per recording: %d' %trial_samples)\n",
    "\n",
    "\n",
    "\n",
    "data_arr=[]\n",
    "for k,v in sorted(Data_align.items())[5:]: # this excludes the first 5 TTL signal recordings\n",
    "    \n",
    "    to_CAR = np.array(v).T      #reshape to be the correct type for applyCAR function\n",
    "    post_CAR = applyCAR(to_CAR) #applies the CAR function here\n",
    "    post_CAR = post_CAR.T       #reshape back to fit the rest of the code\n",
    "    \n",
    "    if post_CAR.shape[0]>=trial_samples:\n",
    "        data_arr.append(post_CAR[:trial_samples,:])\n",
    "    elif post_CAR.shape[0]<trial_samples:\n",
    "        data_arr.append(np.pad(post_CAR,((0,trial_samples-post_CAR.shape[0]),(0,0)), mode='constant', constant_values=0))\n",
    "\n",
    "# for i in data_arr: # this can be used to check that each trial is the correct length and shape\n",
    "#     print(i.shape)\n",
    "\n",
    "data_todat=np.vstack(data_arr)\n",
    "print(\"Final concat shape: {0} -- should be (samp,ch)\".format(data_todat.shape))\n",
    "\n",
    "# Depending on how many trials you have, this may take a while to finish running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double check and make sure everything worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of trials expected: 29\n",
      "# of trials at the end: 29\n",
      "\n",
      "# of ch expected: 384\n",
      "# of ch at the end: 384\n",
      "\n",
      "# of samples/trial expected: 90000\n",
      "# of samples/trial at the end: 90000\n"
     ]
    }
   ],
   "source": [
    "print('# of trials expected: %d' %(len(Data_align)-5))\n",
    "print('# of trials at the end: %d' %(data_todat.shape[0]/(trial_samples)))\n",
    "\n",
    "print('\\n# of ch expected: %d' %(v.shape[1]))\n",
    "print('# of ch at the end: %d' %(data_todat.shape[1]))\n",
    "\n",
    "print('\\n# of samples/trial expected: %d' %(trial_samples))\n",
    "print('# of samples/trial at the end: %d' %(data_todat.shape[0]/(len(Data_align)-5)))\n",
    "\n",
    "\n",
    "if (\n",
    "    len(Data_align)-5 != data_todat.shape[0]/(trial_samples) or \n",
    "    v.shape[1] != data_todat.shape[1] or \n",
    "    trial_samples != data_todat.shape[0]/(len(Data_align)-5)\n",
    "    ):\n",
    "    raise TypeError(\"Something went wrong, check the data again...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, save the new file to a specific location\n",
    "This will create a folder in the specific sorted_folder you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\mz_Data\\RECORDING_DATA\\HDAC_data\\SORTED_DATA\\pre_et306_cc033657_2022-03-15_11-40-00\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "output_path = out_path + '\\\\' + folder_name\n",
    "print(output_path)\n",
    "\n",
    "os.makedirs(output_path) #this makes a new folder with the same file name as above\n",
    "\n",
    "ff = os.path.join(output_path,'openephys.dat')\n",
    "with open(ff,'ab') as fi:\n",
    "    data_todat.astype('int16').tofile(fi)       #This also makes sure that it is \"int16\" data type\n",
    "    print('DONE!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're done running the notebook for the recording!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
