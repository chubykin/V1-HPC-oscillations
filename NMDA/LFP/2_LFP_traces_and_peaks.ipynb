{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# import Python3_OpenOE_AC_map_functions_v1_08_30s as oem\n",
    "import mz_LFP_functions as mz_LFP\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('poster')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "# for publication quality plots\n",
    "def set_pub_plots(pal=sns.blend_palette([\"gray\",\"crimson\", 'cyan', 'magenta', 'purple'  ],5)):\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_palette(pal)\n",
    "    sns.set_context(\"poster\", font_scale=1.5, rc={\"lines.linewidth\": 2.5, \"axes.linewidth\":2.5, 'figure.facecolor': 'white'}) \n",
    "    sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "    # optional, makes markers bigger, too, axes.linewidth doesn't seem to work\n",
    "    plt.rcParams['axes.linewidth'] = 2.5\n",
    "\n",
    "rc_pub={'font.size': 25, 'axes.labelsize': 25, 'legend.fontsize': 25.0, \n",
    "    'axes.titlesize': 25, 'xtick.labelsize': 25, 'ytick.labelsize': 25, \n",
    "    #'axes.color_cycle':pal, # image.cmap - rewritesd the default colormap\n",
    "    'axes.linewidth':2.5, 'lines.linewidth': 2.5,\n",
    "    'xtick.color': 'black', 'ytick.color': 'black', 'axes.edgecolor': 'black','axes.labelcolor':'black','text.color':'black'}\n",
    "# to restore the defaults, call plt.rcdefaults() \n",
    "\n",
    "#set_pub_bargraphs()\n",
    "set_pub_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal=sns.blend_palette(['black','royalblue'],2)\n",
    "sns.palplot(pal)\n",
    "sns.set_palette(pal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_depth = 1000  #change this as appropriate\n",
    "\n",
    "sp_bw_ch = 20/2\n",
    "samples_tr = 7350 #this is based on the shortest #samples in a trial\n",
    "sr = 2500\n",
    "n_chan = 384\n",
    "rec_length = 3.0 #how long is the arduino triggered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, load in the .npy arrays and CC_ls\n",
    "These were creaded and saved using the \"1_saving_LFP_arrays\" jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pre_arr = np.load(r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\lfp_npy\\pre_all.npy\")\n",
    "all_post_arr = np.load(r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\lfp_npy\\post_all.npy\")\n",
    "all_novel_arr = np.load(r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\lfp_npy\\novel_all.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\lfp_npy\\pre_et_ls\"\n",
    "\n",
    "open_file = open(pkl_file, \"rb\")\n",
    "et_ls_pre = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "len(et_ls_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\lfp_npy\\post_et_ls\"\n",
    "\n",
    "open_file = open(pkl_file, \"rb\")\n",
    "et_ls_post = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "len(et_ls_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\lfp_npy\\novel_et_ls\"\n",
    "\n",
    "open_file = open(pkl_file, \"rb\")\n",
    "et_ls_novel = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "len(et_ls_novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the VEP ch trace\n",
    "\n",
    "### First, we have to define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VEP_lines(data_array, et_ls): #use this to separate into nmda & sham groups\n",
    "    nmda = []\n",
    "    sham = []\n",
    "    for i in range(data_array.shape[0]):\n",
    "        nmda_ls = ['et1710', 'et1700', 'et1570', 'et1520', 'et171', 'et170', 'et157', 'et152']\n",
    "        if et_ls[i] in nmda_ls:\n",
    "            nmda.append(data_array[i])\n",
    "        else:\n",
    "            sham.append(data_array[i])\n",
    "\n",
    "    nmda_arr = np.array(nmda)\n",
    "    sham_arr = np.array(sham)\n",
    "\n",
    "    mean_nmda = nmda_arr.mean(axis=0)\n",
    "    mean_sham = sham_arr.mean(axis=0)\n",
    "\n",
    "    print('Group nmda array: {0}'.format(nmda_arr.shape))\n",
    "    print('Group nmda mean: {0}'.format(mean_nmda.shape))\n",
    "\n",
    "    print('Group sham array: {0}'.format(sham_arr.shape))\n",
    "    print('Group sham mean: {0}'.format(mean_sham.shape))\n",
    "\n",
    "    V1_nmda = mean_nmda[0:100, :]\n",
    "    min_nmda = np.where(V1_nmda == np.amin(V1_nmda))\n",
    "    min_ch_nmda = min_nmda[0][0]\n",
    "    print(min_ch_nmda)\n",
    "\n",
    "    V1_sham = mean_sham[0:100, :]\n",
    "    min_sham = np.where(V1_sham == np.amin(V1_sham))\n",
    "    min_ch_sham = min_sham[0][0]\n",
    "    print(min_ch_sham)\n",
    "    \n",
    "    return mean_nmda, mean_sham, min_ch_nmda, min_ch_sham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, apply the function to the three situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre\n",
    "pre_mean_nmda, pre_mean_sham, pre_minch_nmda, pre_minch_sham = VEP_lines(all_pre_arr, et_ls_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post\n",
    "post_mean_nmda, post_mean_sham, post_minch_nmda, post_minch_sham = VEP_lines(all_post_arr, et_ls_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel\n",
    "novel_mean_nmda, novel_mean_sham, novel_minch_nmda, novel_minch_sham = VEP_lines(all_novel_arr, et_ls_novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside, can check individual mice if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mouse_Vep(array, channel, group, plt_title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if group == 'sham':\n",
    "        plt_color = 'grey'\n",
    "    elif group == 'nmda':\n",
    "        plt_color = 'lightblue'\n",
    "    mean_ch_traceA = array[channel,:]\n",
    "    time_arr2_A = np.linspace(0, mean_ch_traceA.shape[0]/sr, mean_ch_traceA.shape[0])\n",
    "    plt.plot(time_arr2_A, mean_ch_traceA, label=group, color=plt_color)\n",
    "    plt.title(plt_title)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('uV')\n",
    "    plt.ylim([-400,300])\n",
    "    plt.axvspan(0.5, 0.7, alpha=0.2, facecolor='b')\n",
    "    plt.show()\n",
    "    \n",
    "def VEP_lines_all_mice(data_array, et_ls):\n",
    "    nmda = []\n",
    "    sham = []\n",
    "    for i in range(data_array.shape[0]):\n",
    "        nmda_ls = ['et1710', 'et1700', 'et1570', 'et1520', 'et171', 'et170', 'et157', 'et152']\n",
    "        if et_ls[i] in nmda_ls:\n",
    "            nmda.append(data_array[i])\n",
    "        else:\n",
    "            sham.append(data_array[i])\n",
    "    nmda_arr = np.array(nmda)\n",
    "    sham_arr = np.array(sham)\n",
    "    return nmda_arr, sham_arr\n",
    "\n",
    "def single_mouse_Veps(big_array, avg_ch, my_group, plt_title):\n",
    "    for mouse, m_arr in enumerate(big_array):\n",
    "        print('Mouse index {}'.format(mouse))\n",
    "        plot_mouse_Vep(m_arr, avg_ch, my_group, plt_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre\n",
    "pre_all_nmda, pre_all_sham = VEP_lines_all_mice(all_pre_arr, et_ls_pre)\n",
    "# Post\n",
    "post_all_nmda, post_all_sham = VEP_lines_all_mice(all_post_arr, et_ls_post)\n",
    "# Novel\n",
    "novel_all_nmda, novel_all_sham = VEP_lines_all_mice(all_novel_arr, et_ls_novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre Training individual mice\n",
    "single_mouse_Veps(pre_all_nmda, pre_minch_nmda, my_group='nmda', plt_title='NMDA - Pre')\n",
    "single_mouse_Veps(pre_all_sham, pre_minch_sham, my_group='sham', plt_title='Sham - Pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Post Training individual mice\n",
    "single_mouse_Veps(post_all_nmda, post_minch_nmda, my_group='nmda', plt_title='NMDA - post')\n",
    "single_mouse_Veps(post_all_sham, post_minch_sham, my_group='sham', plt_title='Sham - post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Novel individual mice\n",
    "single_mouse_Veps(novel_all_nmda, novel_minch_nmda, my_group='nmda', plt_title='NMDA - novel')\n",
    "single_mouse_Veps(novel_all_sham, novel_minch_sham, my_group='sham', plt_title='Sham - novel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot representative Veps for single mice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mouse_Vep(pre_all_nmda[3], pre_minch_nmda, group='nmda', plt_title='nmda - Pre')\n",
    "plot_mouse_Vep(pre_all_sham[4], pre_minch_sham, group='sham', plt_title='sham - Pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mouse_Vep(post_all_nmda[3], post_minch_nmda, group='nmda', plt_title='nmda - post')\n",
    "plot_mouse_Vep(post_all_sham[4], post_minch_sham, group='sham', plt_title='sham - post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mouse_Vep(novel_all_nmda[3], novel_minch_nmda, group='nmda', plt_title='nmda - novel')\n",
    "plot_mouse_Vep(novel_all_sham[4], novel_minch_sham, group='sham', plt_title='sham - novel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otherwise, you can plot the group average graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "pre_mean_ch_traceA = pre_mean_sham[pre_minch_sham,:]\n",
    "pre_time_arr2_A = np.linspace(0, pre_mean_ch_traceA.shape[0]/sr, pre_mean_ch_traceA.shape[0])\n",
    "pre_mean_ch_traceB = pre_mean_nmda[pre_minch_nmda,:]\n",
    "pre_time_arr2_B = np.linspace(0, pre_mean_ch_traceB.shape[0]/sr, pre_mean_ch_traceB.shape[0])\n",
    "\n",
    "plt.plot(pre_time_arr2_A, pre_mean_ch_traceA, label='sham')\n",
    "plt.plot(pre_time_arr2_B, pre_mean_ch_traceB, label='nmda')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.title('Pre-training')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('uV')\n",
    "plt.ylim([-350,250])\n",
    "plt.axvspan(0.5, 0.7, alpha=0.2, facecolor='grey')\n",
    "\n",
    "# Change the end of this next line with a new file name!!\n",
    "# plt.savefig(r\"C:\\Users\\AChub_Lab\\Desktop\\tmp_nmda\\lfp\\pre_VEP_trace.pdf\", transparent=True) # fix this before running\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "post_mean_ch_traceA = post_mean_sham[post_minch_sham,:]\n",
    "post_time_arr2_A = np.linspace(0, post_mean_ch_traceA.shape[0]/sr, post_mean_ch_traceA.shape[0])\n",
    "post_mean_ch_traceB = post_mean_nmda[post_minch_nmda,:]\n",
    "post_time_arr2_B = np.linspace(0, post_mean_ch_traceB.shape[0]/sr, post_mean_ch_traceB.shape[0])\n",
    "\n",
    "plt.plot(post_time_arr2_A, post_mean_ch_traceA, label='sham')\n",
    "plt.plot(post_time_arr2_B, post_mean_ch_traceB, label='nmda')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.title('Post-training')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('uV')\n",
    "plt.ylim([-350,250])\n",
    "plt.axvspan(0.5, 0.7, alpha=0.2, facecolor='grey')\n",
    "\n",
    "# Change the end of this next line with a new file name!!\n",
    "# plt.savefig(r\"C:\\Users\\AChub_Lab\\Desktop\\tmp_nmda\\lfp\\post_VEP_trace.pdf\", transparent=True) # fix this before running\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "novel_mean_ch_traceA = novel_mean_sham[novel_minch_sham,:]\n",
    "novel_time_arr2_A = np.linspace(0, novel_mean_ch_traceA.shape[0]/sr, novel_mean_ch_traceA.shape[0])\n",
    "novel_mean_ch_traceB = novel_mean_nmda[novel_minch_nmda,:]\n",
    "novel_time_arr2_B = np.linspace(0, novel_mean_ch_traceB.shape[0]/sr, novel_mean_ch_traceB.shape[0])\n",
    "\n",
    "plt.plot(novel_time_arr2_A, novel_mean_ch_traceA, label='sham')\n",
    "plt.plot(novel_time_arr2_B, novel_mean_ch_traceB, label='nmda')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.title('Novel')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('uV')\n",
    "plt.ylim([-350,250])\n",
    "plt.axvspan(0.5, 0.7, alpha=0.2, facecolor='grey')\n",
    "\n",
    "# Change the end of this next line with a new file name!!\n",
    "# plt.savefig(r\"C:\\Users\\AChub_Lab\\Desktop\\tmp_nmda\\lfp\\novel_VEP_trace.pdf\", transparent=True) # fix this before running\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(post_time_arr2_A, post_mean_ch_traceA, label='sham')\n",
    "plt.plot(post_time_arr2_B, post_mean_ch_traceB, label='nmda')\n",
    "\n",
    "plt.axvspan(0.55, 0.77, alpha=0.2)\n",
    "plt.axvspan(0.78, 0.95, alpha=0.2)\n",
    "plt.axvspan(0.96, 1.2, alpha=0.2)\n",
    "plt.axvspan(1.21, 1.45, alpha=0.2)\n",
    "plt.axvspan(1.46, 1.8, alpha=0.2)\n",
    "\n",
    "plt.xlim([0.5,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the VEP peaks in each situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_win=[[0.55, 0.77],   # VEP 1 time frame\n",
    "          [0.78, 0.95],  # VEP 2 time frame\n",
    "          [0.96, 1.2],     # VEP 3 time frame\n",
    "          [1.21, 1.45],  # VEP 4 time frame\n",
    "          [1.46, 1.8]]    # VEP 5 time frame\n",
    "\n",
    "\n",
    "time_win=[[i[0]*sr,i[1]*sr] for i in time_win]\n",
    "depth_b=[[0,100],[101,102]] #V1 bounds and hippocampus/Thalamus bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_V1vepdf = mz_LFP.create_vepamp_df(all_pre_arr, time_windows=time_win, depth_bounds=depth_b)\n",
    "post_V1vepdf = mz_LFP.create_vepamp_df(all_post_arr, time_windows=time_win, depth_bounds=depth_b)\n",
    "novel_V1vepdf = mz_LFP.create_vepamp_df(all_novel_arr, time_windows=time_win, depth_bounds=depth_b)\n",
    "\n",
    "# pre_V1vepdf = mz_LFP.create_vep_df(all_pre_arr, time_windows=time_win, depth_bounds=depth_b)\n",
    "# post_V1vepdf = mz_LFP.create_vep_df(all_post_arr, time_windows=time_win, depth_bounds=depth_b)\n",
    "# novel_V1vepdf = mz_LFP.create_vep_df(all_novel_arr, time_windows=time_win, depth_bounds=depth_b)\n",
    "\n",
    "# r1_V1vepdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novel_V1vepdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second, we add in some additonal info to the dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_ls = et_ls_pre\n",
    "\n",
    "et_dfls = []\n",
    "groupls = []\n",
    "for i in pre_V1vepdf.rec_ind:\n",
    "    et_dfls.append(et_ls[i])\n",
    "    nmda_ls = ['et1710', 'et1700', 'et1570', 'et1520', 'et171', 'et170', 'et157', 'et152']\n",
    "    if et_ls[i] in nmda_ls:\n",
    "        groupls.append(\"nmda\")\n",
    "    else:\n",
    "        groupls.append(\"sham\")\n",
    "\n",
    "pre_V1vepdf[\"et\"] = et_dfls\n",
    "pre_V1vepdf[\"group\"] = groupls\n",
    "pre_V1vepdf[\"stage\"] = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_ls = et_ls_post\n",
    "\n",
    "et_dfls = []\n",
    "groupls = []\n",
    "for i in post_V1vepdf.rec_ind:\n",
    "    et_dfls.append(et_ls[i])\n",
    "    nmda_ls = ['et1710', 'et1700', 'et1570', 'et1520', 'et171', 'et170', 'et157', 'et152']\n",
    "    if et_ls[i] in nmda_ls:\n",
    "        groupls.append(\"nmda\")\n",
    "    else:\n",
    "        groupls.append(\"sham\")\n",
    "\n",
    "post_V1vepdf[\"et\"] = et_dfls\n",
    "post_V1vepdf[\"group\"] = groupls\n",
    "post_V1vepdf[\"stage\"] = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_ls = et_ls_novel\n",
    "\n",
    "et_dfls = []\n",
    "groupls = []\n",
    "for i in novel_V1vepdf.rec_ind:\n",
    "    et_dfls.append(et_ls[i])\n",
    "    nmda_ls = ['et1710', 'et1700', 'et1570', 'et1520', 'et171', 'et170', 'et157', 'et152']\n",
    "    if et_ls[i] in nmda_ls:\n",
    "        groupls.append(\"nmda\")\n",
    "    else:\n",
    "        groupls.append(\"sham\")\n",
    "\n",
    "novel_V1vepdf[\"et\"] = et_dfls\n",
    "novel_V1vepdf[\"group\"] = groupls\n",
    "novel_V1vepdf[\"stage\"] = 'novel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training = pd.concat([pre_V1vepdf, post_V1vepdf, novel_V1vepdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we can plot the quantification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=all_training,\n",
    "            x='vep',\n",
    "            y='vepamp',\n",
    "            kind='point',\n",
    "            hue='group',\n",
    "            col='stage',\n",
    "            ci=68,\n",
    "            legend=False,\n",
    "            hue_order=['sham','nmda'],\n",
    "            height = 5,\n",
    "            aspect=1.2\n",
    "           )\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.ylabel('uV')\n",
    "plt.xlabel('Oscillation Cycle')\n",
    "# plt.ylim([0,500])\n",
    "# plt.yticks([0,250,500])\n",
    "\n",
    "# Change the end of this next line with a new file name!!\n",
    "# plt.savefig(r\"C:\\Users\\AChub_Lab\\Desktop\\tmp_nmda\\lfp\\VEPamp_quant.pdf\", transparent=True) # fix this before running\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the stats about the above plots\n",
    "### Overall test for the genotypes (wt vs dko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_stats, pre_fstats = mz_LFP.VEP_overall_stats(pre_V1vepdf, group_ls=['sham','nmda'])\n",
    "post_stats, post_fstats = mz_LFP.VEP_overall_stats(post_V1vepdf, group_ls=['sham','nmda'])\n",
    "novel_stats, novel_fstats = mz_LFP.VEP_overall_stats(novel_V1vepdf, group_ls=['sham','nmda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('~~~~~ Pre training ~~~~~')\n",
    "print(pre_stats)\n",
    "print(pre_fstats)\n",
    "print('~~~~~ Post training ~~~~~')\n",
    "print(post_stats)\n",
    "print(post_fstats)\n",
    "print('~~~~~ Novel ~~~~~')\n",
    "print(novel_stats)\n",
    "print(novel_fstats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual tests for each vep peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepeak_stats, prepeak_fstats = mz_LFP.VEP_peaks_stats(pre_V1vepdf, group_ls=['sham','nmda'])\n",
    "postpeak_stats, postpeak_fstats = mz_LFP.VEP_peaks_stats(post_V1vepdf, group_ls=['sham','nmda'])\n",
    "novelpeak_stats, novelpeak_fstats = mz_LFP.VEP_peaks_stats(novel_V1vepdf, group_ls=['sham','nmda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('~~~~~ Pre training ~~~~~')\n",
    "print(prepeak_stats)\n",
    "print(prepeak_fstats)\n",
    "print('~~~~~ Post training ~~~~~')\n",
    "print(postpeak_stats)\n",
    "print(postpeak_fstats)\n",
    "print('~~~~~ Novel ~~~~~')\n",
    "print(novelpeak_stats)\n",
    "print(novelpeak_fstats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Mixed Model analysis\n",
    "First, for the Amp values x Vep # with a comparison between groups\n",
    "\n",
    "Second, for the Amp values x group with a comparison between Vep #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pre_mdf1, pre_mdf2 = mz_LFP.LMM_stats(pre_V1vepdf)\n",
    "print(pre_mdf1.pvalues)\n",
    "print(pre_mdf2.pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "post_mdf1, post_mdf2 = mz_LFP.LMM_stats(post_V1vepdf)\n",
    "print(post_mdf1.pvalues)\n",
    "print(post_mdf2.pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "novel_mdf1, novel_mdf2 = mz_LFP.LMM_stats(novel_V1vepdf)\n",
    "print(novel_mdf1.pvalues)\n",
    "print(novel_mdf2.pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new method for determining the peaks, just get the troughs and values from that, don't use time windows\n",
    "import scipy.signal as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_vep_amp(all_array, et_ls, training_stage):\n",
    "    big_df_ls = []\n",
    "    for mi,mouse in enumerate(all_array):\n",
    "        min_arr = np.where(mouse[50:80, 1250:5000] == np.amin(mouse[50:80, 1250:5000]))\n",
    "        min_ch = min_arr[0][0]+50\n",
    "\n",
    "        mouse_vep = mouse[min_ch, 1250:5000] #find the VEP channel\n",
    "\n",
    "        #find the idx of the peaks and troughs & find the associated value for each\n",
    "        inv_vep = -mouse_vep\n",
    "        troughs, _ = ss.find_peaks(inv_vep, distance=100, width=20, prominence=40)\n",
    "        peaks, _ = ss.find_peaks(mouse_vep, distance=100, width=20, prominence=40)\n",
    "        \n",
    "        plt.plot(mouse_vep)\n",
    "        plt.plot(peaks, mouse_vep[peaks], \"x\")\n",
    "        plt.plot(troughs, mouse_vep[peaks], \"x\")\n",
    "        \n",
    "#         tr_values = []\n",
    "#         for idx in troughs:\n",
    "#             val = mouse_vep[idx]\n",
    "#             tr_values.append(val)\n",
    "#         pk_values = []\n",
    "#         for idx in peaks:\n",
    "#             val = mouse_vep[idx]\n",
    "#             pk_values.append(val)\n",
    "#         #calculate the amplitude of each trough-peak change\n",
    "#         tr_pk = []\n",
    "#         pk_idx = []\n",
    "#         for ii, t_val in enumerate(tr_values):\n",
    "#             try:\n",
    "#                 p_val = pk_values[ii]\n",
    "#             except:\n",
    "#                 p_val = 0\n",
    "#             tr_pk.append(abs(val - p_val))\n",
    "#             pk_idx.append(ii+1)\n",
    "\n",
    "#         #get metadata for the df below\n",
    "#         nmda_ls = ['et1710', 'et1700', 'et1570', 'et1520', 'et171', 'et170', 'et157', 'et152']\n",
    "#         if et_ls[mi] in nmda_ls:\n",
    "#             group = \"nmda\"\n",
    "#         else:\n",
    "#             group = \"sham\"\n",
    "\n",
    "#         #make dataframe of vep quantification\n",
    "#         mouse_df = pd.DataFrame({'vep':pk_idx,'vepamp':tr_pk,'et':et_ls[mi], 'group':group, 'stage':training_stage})    \n",
    "#         big_df_ls.append(mouse_df)\n",
    "\n",
    "#     post_veps = pd.concat(big_df_ls)\n",
    "    \n",
    "#     return post_veps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_veps_df = new_vep_amp(all_pre_arr, et_ls_pre, training_stage='pre')\n",
    "post_veps_df = new_vep_amp(all_post_arr, et_ls_post, training_stage='post')\n",
    "novel_veps_df = new_vep_amp(all_novel_arr, et_ls_novel, training_stage='novel')\n",
    "\n",
    "all_training_veps = pd.concat([pre_veps_df, post_veps_df, novel_veps_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=all_training_veps,\n",
    "            x='vep',\n",
    "            y='vepamp',\n",
    "            kind='point',\n",
    "            hue='group',\n",
    "            col='stage',\n",
    "            ci=68,\n",
    "            legend=False,\n",
    "            hue_order=['sham','nmda'],\n",
    "            height = 4.5,\n",
    "            aspect=1.5\n",
    "           )\n",
    "\n",
    "# plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Change the end of this next line with a new file name!!\n",
    "# plt.savefig(r\"D:\\mz_Data\\DATA_Figs\\HDAC\\LFP\\pre_VEP_quant.pdf\", transparent=True) # fix this before running\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
