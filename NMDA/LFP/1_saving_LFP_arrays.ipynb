{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('poster')\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "# import Python3_OpenOE_AC_map_functions_v1_08_30s as oem\n",
    "import mz_LFP_functions as mz_LFP\n",
    "\n",
    "import matplotlib.animation as ani #this was a good idea that I didn't want to spend time on right now\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load some necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_depth = 1000  #change this as appropriate\n",
    "sp_bw_ch = 20/2\n",
    "\n",
    "samples_tr = 7350 #this is based on the shortest #samples in a trial\n",
    "sr = 2500\n",
    "n_chan = 384\n",
    "rec_length = 3.0 #how long is the arduino triggered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For multiple mice\n",
    "Requires a __`user input`__ to choose the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario (pre, post, novel): novel\n"
     ]
    }
   ],
   "source": [
    "data_choice = input('Scenario (pre, post, novel): ')\n",
    "\n",
    "if data_choice == 'pre':\n",
    "    start_path_ls=glob.glob(r\"G:/Neuropixels/NMDA_V1HPC/\"+'pre*')     # pre-training to the visual stim\n",
    "    total_rec = 34\n",
    "elif data_choice == 'post':\n",
    "    start_path_ls=glob.glob(r\"G:/Neuropixels/NMDA_V1HPC/\"+'post*')     # post-training to the visual stim\n",
    "    total_rec = 34\n",
    "elif data_choice == 'novel':\n",
    "    start_path_ls=glob.glob(r\"G:/Neuropixels/NMDA_V1HPC/\"+'novel*')     # novel stimulus\n",
    "    total_rec = 34\n",
    "else:\n",
    "    raise Exception('Input is not one of the options (pre, post, novel)')\n",
    "\n",
    "\n",
    "end_path = r\"\\continuous\\Neuropix-PXI-100.1\\continuous.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['et1710',\n",
       " 'et171',\n",
       " 'et1700',\n",
       " 'et170',\n",
       " 'et152',\n",
       " 'et2750',\n",
       " 'et275',\n",
       " 'et630',\n",
       " 'et63',\n",
       " 'et2760',\n",
       " 'et276']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldatals=[]\n",
    "et_ls = []\n",
    "for start_path in start_path_ls:\n",
    "    ls = []\n",
    "    et = start_path.split('\\\\')[-1].split('_')[2]\n",
    "    \n",
    "    for i in range(6, total_rec): # remember, you have to exclude the first short TTL signal trials\n",
    "        exp_rec_path = rf\"\\experiment1\\recording{i}\"\n",
    "        fileName = start_path + exp_rec_path + end_path\n",
    "        data = np.memmap(fileName, dtype='int16', mode='c')\n",
    "        data2 = data.reshape(-1, n_chan)\n",
    "        ls.append(data2[:samples_tr, 0:384])\n",
    "        \n",
    "    alldatals.append(ls)\n",
    "    et_ls.append(et)\n",
    "\n",
    "# alldatals: num_mice x num_trials x num_samples x num_ch\n",
    "num_mice = len(et_ls)\n",
    "print(num_mice)\n",
    "et_ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the lists into CAR filtered arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(0, samples_tr/sr, samples_tr)\n",
    "scale_factor = 0.195\n",
    "time_arr = [0,0.5,1,1.5,2,2.5]\n",
    "time_plot = [i*sr for i in time_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses \"applyCAR\" and \"notch_filt\" functions\n",
    "def create_all_arr(alldatals):\n",
    "    novel_ls=[]\n",
    "    for i in range(len(alldatals)):\n",
    "        tmp2 = alldatals[i]                               # getting the individual trials\n",
    "        tmp3_ls = []\n",
    "        for ii in range(len(tmp2)):\n",
    "            tmp3 = tmp2[ii]\n",
    "            tmp3 = tmp3.T\n",
    "            filt_tmp3 = []\n",
    "            for ch in range(tmp3.shape[0]):\n",
    "                ch_data = tmp3[ch,:]\n",
    "                ch_notc_data = mz_LFP.notch_filt(ch_data)\n",
    "                filt_tmp3.append(ch_notc_data)\n",
    "            filt_tmp3 = np.array(filt_tmp3)\n",
    "            CARfilt_tmp3 = mz_LFP.applyCAR(filt_tmp3, pr=0)\n",
    "            scaled_CARfilt_tmp3 = CARfilt_tmp3*scale_factor\n",
    "            tmp3_ls.append(scaled_CARfilt_tmp3)\n",
    "        novel_ls.append(tmp3_ls)\n",
    "        print('Loaded {0}'.format(i))\n",
    "    final_arr = np.array(novel_ls)\n",
    "    return final_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses \"applyCAR\" and \"notch_filt\" functions\n",
    "def create_novel_arr(alldatals):\n",
    "    novel_ls=[]\n",
    "    for i in range(len(alldatals)):\n",
    "        tmp2 = np.mean(alldatals[i], axis = 0)           # getting the mean traces over all trials\n",
    "        tmp2 = tmp2.T\n",
    "        filt_tmp2 = []\n",
    "        for ch in range(tmp2.shape[0]):\n",
    "            ch_data = tmp2[ch,:]\n",
    "            ch_notc_data = mz_LFP.notch_filt(ch_data)\n",
    "            filt_tmp2.append(ch_notc_data)\n",
    "        filt_tmp2 = np.array(filt_tmp2)\n",
    "        CARfilt_tmp2 = mz_LFP.applyCAR(filt_tmp2, pr=0)\n",
    "        scaled_CARfilt_tmp2 = CARfilt_tmp2*scale_factor\n",
    "        novel_ls.append(scaled_CARfilt_tmp2)\n",
    "        print('Loaded {0}'.format(i))\n",
    "    final_arr = np.array(novel_ls)\n",
    "    return final_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldatals) # # mice x # trials x # samples x # channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0\n",
      "Loaded 1\n",
      "Loaded 2\n",
      "Loaded 3\n",
      "Loaded 4\n",
      "Loaded 5\n",
      "Loaded 6\n",
      "Loaded 7\n",
      "Loaded 8\n",
      "Loaded 9\n",
      "Loaded 10\n"
     ]
    }
   ],
   "source": [
    "all_arr = create_all_arr(alldatals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 28, 384, 7350)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the LFP arrays and CC_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "arr_save_path = r\"D:\\mz_Data\\saved_dfs\\HPC_nmda\\lfp_npy\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_choice == 'pre':\n",
    "    fn1 = r\"pre_all_trials\"\n",
    "    out1 = arr_save_path + \"\\\\\" + fn1\n",
    "    np.save(out1, all_arr)         # saving pre array\n",
    "    pkl_fn = r\"pre_et_ls\"\n",
    "elif data_choice == 'post':\n",
    "    fn1 = r\"post_all_trials\"\n",
    "    out1 = arr_save_path + \"\\\\\" + fn1\n",
    "    np.save(out1, all_arr)         # saving post array\n",
    "    pkl_fn = r\"post_et_ls\"\n",
    "elif data_choice == 'novel':\n",
    "    fn1 = r\"novel_all_trials\"\n",
    "    out1 = arr_save_path + \"\\\\\" + fn1\n",
    "    np.save(out1, all_arr)         # saving novel array\n",
    "    pkl_fn = r\"novel_et_ls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_out = arr_save_path + \"\\\\\" + pkl_fn\n",
    "\n",
    "open_file = open(pkl_out, \"wb\")\n",
    "pickle.dump(et_ls, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
