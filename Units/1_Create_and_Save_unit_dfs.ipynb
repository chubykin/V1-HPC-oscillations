{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "import scipy.stats as sstat\n",
    "import scipy.signal as ssig\n",
    "import h5py\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import re\n",
    "\n",
    "# import ephys_unit_analysis as ena\n",
    "import mz_ephys_unit_analysis as mz_ena\n",
    "\n",
    "#import resampy\n",
    "import fnmatch\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NP probe inserted through V1 and hippo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = 'Neuropixels'\n",
    "channel_groups = mz_ena.get_channel_depth(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files = []\n",
    "matches = [] # list of experiment folders\n",
    "source_folder = r\"G:\\Neuropixels\\Sam_multi_brain_region_paper\\SORTED\"\n",
    "\n",
    "for root, dirnames, filenames in os.walk(source_folder):\n",
    "    for filename in fnmatch.filter(filenames, '*rez.mat'):\n",
    "        for filename in fnmatch.filter(filenames, '*cluster_group.tsv'):#For newer phy2 GUI, .tsv instead of .csv files\n",
    "            if (str('novel') in root):\n",
    "                if (str('bad') not in root):\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "                    root_files.append(root)\n",
    "                    print (root)\n",
    "\n",
    "print('\\nIMPORTANT: This has \"cluster_group.tsv\" already appended to the matches list')\n",
    "print ('How many files?',len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_units_or_good = 1   # if 0--manually sorted good units, if 1--all units from KS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = []\n",
    "df_rez = []\n",
    "\n",
    "for f in root_files:\n",
    "    path = f\n",
    "    cluster_path = os.path.join(path, 'cluster_KSLabel.tsv')\n",
    "\n",
    "    stim_type = f.split('\\\\')[-1].split('_')[0]\n",
    "    et_num = f.split('\\\\')[-1].split('_')[2]\n",
    "    cc_num = f.split('\\\\')[-1].split('_')[1]   # what cage number is this from?\n",
    "    \n",
    "    cluster_groups = pd.read_csv(cluster_path, sep = '\\t')\n",
    "    \n",
    "    if all_units_or_good == 0:\n",
    "        good = cluster_groups[cluster_groups['group'] == 'good'].cluster_id.values\n",
    "    elif all_units_or_good == 1:\n",
    "        good = cluster_groups[cluster_groups['KSLabel'] == 'good'].cluster_id.values\n",
    "    \n",
    "    spike_clusters = np.load(os.path.join(path, 'spike_clusters.npy'))\n",
    "    spike_times = np.load(os.path.join(path, 'spike_times.npy'))\n",
    "    templates = np.load(os.path.join(path, 'templates.npy'))\n",
    "    spike_templates = np.load(os.path.join(path, 'spike_templates.npy'))\n",
    "\n",
    "    foo = pd.DataFrame({'stim_type': stim_type,\n",
    "                            'et': et_num,\n",
    "                            'cc': cc_num,\n",
    "                            'cluster_id': spike_clusters.flatten(),\n",
    "                            'times': spike_times.flatten()/30000.0, \n",
    "                            'templates': spike_templates.flatten(),\n",
    "                            'path': f})\n",
    "    \n",
    "    data_df.append(foo)\n",
    "    \n",
    "    foo_1 = foo[foo.cluster_id.isin(good)]\n",
    "    df_rez.append(foo_1)\n",
    "\n",
    "data_df = pd.concat(data_df, axis=0, ignore_index=True)\n",
    "df_rez = pd.concat(df_rez, axis=0, ignore_index=True)\n",
    "\n",
    "print('total units df shape:', data_df.shape)\n",
    "print('\"good\" units df shape:', df_rez.shape)\n",
    "\n",
    "print('Total paths:', df_rez.path.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['cuid'] =  data_df.et.astype(str) + str('_') + data_df.cluster_id.astype(str)\n",
    "df_rez['cuid'] =  df_rez.et.astype(str) + str('_') + df_rez.cluster_id.astype(str)\n",
    "\n",
    "print(\"Total units:\", data_df['cuid'].nunique())\n",
    "print(\"Good units:\", df_rez['cuid'].nunique())\n",
    "\n",
    "df_rez.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep going from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trials_number = 50 # ~~~~~~~~~~~~~~~~~~~~~~IMPORTANT TO CHANGE THIS~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "trial_length = 3.0 #this is the length of the recording in OpenEphys (the yellow highlight)\n",
    "th_bin = 0.01\n",
    "\n",
    "ls_rawcount = []\n",
    "ls_lowspikecount = []\n",
    "ls_refract_violators = []\n",
    "ls_lowamp_waveforms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['trial']=(data_df.times//trial_length).astype(int)\n",
    "df_rez['trial']=(df_rez.times//trial_length).astype(int)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the dataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_spikes = []\n",
    "ls_tmt = []\n",
    "ls_psth = []\n",
    "\n",
    "i=0\n",
    "num_units = df_rez['cuid'].nunique()\n",
    "\n",
    "for iii, unit in enumerate(df_rez['cuid'].unique()): ##### I changed this from df_rez to data_df to check the units\n",
    "    cuid = str(unit)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    tmp2 = df_rez[(df_rez.cuid == unit)]             ##### I changed this from df_rez to data_df to check the units\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    stim_id = tmp2.stim_type.values[0]\n",
    "    cluster_id = tmp2.cluster_id.values[0]\n",
    "    et = tmp2.et.values[0]\n",
    "    cc = tmp2.cc.values[0]\n",
    "    path = tmp2.path.values[0]\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    try:\n",
    "        tmt, depth, ch_idx = mz_ena.ksort_get_tmt(tmp2, cluster_id, templates, channel_groups)\n",
    "    except:\n",
    "        i = i+1        \n",
    "        continue    \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    df = mz_ena.getRaster_kilosort(tmp2, unit, trial_length) \n",
    "    trials_number_not_empty = len(df.trial.unique())    \n",
    "\n",
    "    h, ttr = mz_ena.PSTH(df.times, th_bin, trial_length, trials_number_not_empty)\n",
    "\n",
    "    zscore = sstat.mstats.zscore(h)\n",
    "    mean = np.mean(h[0:50])#The Baseline period. Be sure it matches time course of experiments##\n",
    "    if mean<=0:\n",
    "        std=1\n",
    "    else:\n",
    "        std = np.std(h[0:50])#The Baseline period. Be sure it matches time course of experiments##\n",
    "    ztc = (h - mean)/std\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    if iii%200 == 0:\n",
    "        print('done with {0} out of {1}'.format(iii, num_units))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    df_psth_tmp = pd.DataFrame({'times':ttr,\n",
    "                                'stim': stim_id,\n",
    "                                'Hz':h,\n",
    "                                'cluster_id': cluster_id,\n",
    "                                'depth': depth,\n",
    "                                'zscore':zscore, \n",
    "                                'ztc':ztc,\n",
    "                                'et':et,\n",
    "                                'cc': cc,\n",
    "                                'cuid':cuid,\n",
    "                                'path':path})\n",
    "    ls_psth.append(df_psth_tmp)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    df_spikes_tmp = pd.DataFrame({'cluster_id': cluster_id, \n",
    "                                  'spikes': tmp2.times.values,\n",
    "                                  'trial':df.trial,\n",
    "                                  'trial_spikes':df.times,\n",
    "                                  'stim': stim_id,\n",
    "                                  'depth':depth,\n",
    "                                  'et':et,\n",
    "                                  'cc': cc,\n",
    "                                  'cuid': cuid,\n",
    "                                  'path':path})\n",
    "    ls_spikes.append(df_spikes_tmp)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    df_tmt_tmp = pd.DataFrame({'tmt': tmt,\n",
    "                               'stim': stim_id,\n",
    "                               'depth':depth,\n",
    "                               'et':et,\n",
    "                               'cc': cc,\n",
    "                               'cuid': cuid,\n",
    "                               'path':path})\n",
    "    ls_tmt.append(df_tmt_tmp)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print(i, \"errors\")\n",
    "df_psth = pd.concat(ls_psth)\n",
    "df_spikes = pd.concat(ls_spikes)\n",
    "df_tmt = pd.concat(ls_tmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min unit depth on probe:', df_psth['depth'].min())\n",
    "print('Max unit depth on probe:', df_psth['depth'].max())\n",
    "\n",
    "# print('\\n',np.sort(df_psth['depth'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df = df_psth[df_psth['zscore'] < 20]\n",
    "\n",
    "print('# good units: %d' % df_psth['cuid'].nunique())\n",
    "print('# good units w/ z-score < 20: %d' % null_df['cuid'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the dfs with group and region labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_et_group(row):\n",
    "#     if (row['et']==\"et1\")|(row['et']==\"et2\")|(row['et']==\"et10\")|(row['et']==\"et20\")|(row['et']==\"et200\")|(row['et']==\"et30\")|(row['et']==\"et3\"):\n",
    "#         return \"wt\"\n",
    "#     else:\n",
    "#         return \"no_con\"\n",
    "\n",
    "# def label_group(row):\n",
    "#     if (row['cc'] == \"CC082263\") | (row['cc'] == \"CC067489\") | (row['cc'] == \"CC082260\") | (row['cc'] == \"CC084621\"):\n",
    "#         return \"wt\"\n",
    "#     elif (row['cc'] == \"CC082257\") | (row['cc'] == \"CC067431\") | (row['cc'] == \"CC067432\") | (row['cc'] == \"CC082255\"):\n",
    "#         return \"fx\"\n",
    "    \n",
    "def label_region(row):\n",
    "    if (row['depth'] <= 3100) & (row['depth'] >= 2000):\n",
    "        return 'v1'\n",
    "    elif (row['depth'] < 1800) & (row['depth'] >= 600):\n",
    "        return 'hippo'\n",
    "    elif (row['depth'] < 600):\n",
    "        return 'thal'\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_df['region'] = null_df.apply(lambda row: label_region(row), axis=1)\n",
    "\n",
    "null_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spikes['region'] = df_spikes.apply(lambda row: label_region(row), axis=1)\n",
    "\n",
    "df_spikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmt['group'] = df_tmt.apply(lambda row: label_et_group(row), axis=1)\n",
    "df_tmt['region'] = df_tmt.apply(lambda row: label_region(row), axis=1)\n",
    "\n",
    "df_tmt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last reordering of the columns for easy viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a last reordering of the columns for easy viewing\n",
    "cols = ['times', 'cuid', 'depth', 'Hz', 'zscore', 'ztc', 'region', 'stim', 'cc', 'et', 'cluster_id', 'path']\n",
    "null_df = null_df[cols]\n",
    "\n",
    "null_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['trial', 'trial_spikes', 'spikes', 'cuid', 'stim', 'depth', 'region', 'et', 'cc', 'cluster_id', 'path']\n",
    "df_spikes = df_spikes[cols]\n",
    "\n",
    "df_spikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['tmt', 'stim', 'region', 'depth', 'et', 'cc', 'cuid', 'path']\n",
    "df_tmt = df_tmt[cols]\n",
    "\n",
    "df_tmt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only keep the WT pre & post & novel (?) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_psth = null_df[null_df.group.isin(['wt'])]\n",
    "# df_spikes = df_spikes[df_spikes.group.isin(['wt'])]\n",
    "# df_tmt = df_tmt[df_tmt.group.isin(['wt'])]\n",
    "\n",
    "# print(df_psth.group.unique())\n",
    "# print(df_spikes.group.unique())\n",
    "# print(df_tmt.group.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataframe and plot elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_psth.to_pickle(r\"D:\\mz_Data\\saved_dfs\\Multi_brain_regions\\redo_brain_regions\\V1HPC_novel_psth.pkl\")\n",
    "df_spikes.to_pickle(r\"D:\\mz_Data\\saved_dfs\\Multi_brain_regions\\redo_brain_regions\\V1HPC_novel_spikes.pkl\")\n",
    "df_tmt.to_pickle(r\"D:\\mz_Data\\saved_dfs\\Multi_brain_regions\\redo_brain_regions\\V1HPC_novel_waveforms.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
